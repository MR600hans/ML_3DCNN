{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5898d870",
   "metadata": {},
   "source": [
    "# PyTorch 3DResNet Classfication\n",
    "This notebook has been converted to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decive: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"decive: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'structured_data_mac.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查並清理數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['Group'].isnull().any():\n",
    "    df = df.dropna(subset=['Group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查是否存在非預期的標籤值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_labels = ['CN', 'MCI', 'AD']\n",
    "df = df[df['Group'].isin(expected_labels)]\n",
    "\n",
    "image_paths = df['Image Path'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將標籤映射為數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
    "labels = df['Group'].map(label_mapping).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 獲取獨特的病人ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_patients = df['Subject'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根據病人ID進行訓練和驗證分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(unique_patients, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 創建訓練和驗證DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['Subject'].isin(train_ids)]\n",
    "val_df = df[df['Subject'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取訓練和驗證影像路徑和標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = train_df['Image Path'].tolist()\n",
    "train_labels = train_df['Group'].map(label_mapping).tolist()\n",
    "val_paths = val_df['Image Path'].tolist()\n",
    "val_labels = val_df['Group'].map(label_mapping).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將標籤轉換為 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti_file(nifti_path):\n",
    "    nifti_image = nib.load(nifti_path)\n",
    "    return nifti_image.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    processed_image = np.resize(image, (96, 96, 96))\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_paths, labels, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "\n",
    "            batch_images = [preprocess_image(\n",
    "                load_nifti_file(path)) for path in batch_paths]\n",
    "            yield np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch dataset class\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        nifti_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        nifti_image = nib.load(nifti_path)\n",
    "        image = nifti_image.get_fdata()\n",
    "        image = np.resize(image, (96, 96, 96))\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = image.unsqueeze(0)  # Add channel dimension\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = NiftiDataset(train_paths, train_labels_tensor)\n",
    "val_dataset = NiftiDataset(val_paths, val_labels_tensor)\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立並編譯3D CNN模型\n",
    "\n",
    "此段代碼顯示了如何建立並編譯一個用於二元分類的3D卷積神經網絡（CNN）模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定義 3D ResNet 模型\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3):\n",
    "        self.in_channels = 64\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.in_channels, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, planes, stride, downsample))\n",
    "        self.in_channels = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定義模型的殘差塊\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, channels, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# 實例化 3D ResNet 模型\n",
    "model = ResNet3D(BasicBlock, [3, 4, 6, 3], num_classes=3).to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Conv3DNet(96, 96, 96).to('cuda')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練3D CNN模型\n",
    "\n",
    "此段代碼展示了如何訓練先前建立的3D CNN模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 159/159 [06:35<00:00,  2.49s/it]\n",
      "Validation Epoch 1/10: 100%|██████████| 41/41 [00:11<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.0634147644792713, Validation Loss: 1.4445612241582173\n",
      "Training Accuracy: 0.4805194805194805, Training F1: 0.4365877257243443\n",
      "Validation Accuracy: 0.14152410575427682, Validation F1: 0.03509180823879888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 159/159 [01:19<00:00,  2.00it/s]\n",
      "Validation Epoch 2/10: 100%|██████████| 41/41 [00:11<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.9731184116699411, Validation Loss: 1.430737472162014\n",
      "Training Accuracy: 0.5261707988980716, Training F1: 0.4947681008638093\n",
      "Validation Accuracy: 0.24883359253499224, Validation F1: 0.21019713336976167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 159/159 [01:19<00:00,  2.00it/s]\n",
      "Validation Epoch 3/10: 100%|██████████| 41/41 [00:11<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.9383655377903825, Validation Loss: 2.1917380210829944\n",
      "Training Accuracy: 0.5316804407713499, Training F1: 0.5074790862837102\n",
      "Validation Accuracy: 0.14152410575427682, Validation F1: 0.03509180823879888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 159/159 [01:19<00:00,  2.01it/s]\n",
      "Validation Epoch 4/10: 100%|██████████| 41/41 [00:11<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.9047954299914762, Validation Loss: 2.091274472271524\n",
      "Training Accuracy: 0.5694608421881149, Training F1: 0.5522874358523199\n",
      "Validation Accuracy: 0.15085536547433903, Validation F1: 0.053771761615002495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 159/159 [01:14<00:00,  2.13it/s]\n",
      "Validation Epoch 5/10: 100%|██████████| 41/41 [00:09<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.8868662624988916, Validation Loss: 0.9121630249953852\n",
      "Training Accuracy: 0.5690672963400236, Training F1: 0.5503533947612477\n",
      "Validation Accuracy: 0.5738724727838258, Validation F1: 0.4796798671152757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 159/159 [01:10<00:00,  2.27it/s]\n",
      "Validation Epoch 6/10: 100%|██████████| 41/41 [00:09<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.8373172166962294, Validation Loss: 0.9967853405126711\n",
      "Training Accuracy: 0.5962219598583235, Training F1: 0.5829845286224304\n",
      "Validation Accuracy: 0.5287713841368584, Validation F1: 0.5130932991910359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 159/159 [01:09<00:00,  2.28it/s]\n",
      "Validation Epoch 7/10: 100%|██████████| 41/41 [00:09<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.778077818687607, Validation Loss: 2.8166375814414604\n",
      "Training Accuracy: 0.6387249114521841, Training F1: 0.6273881666046895\n",
      "Validation Accuracy: 0.18973561430793157, Validation F1: 0.12404811804872523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 159/159 [01:10<00:00,  2.27it/s]\n",
      "Validation Epoch 8/10: 100%|██████████| 41/41 [00:09<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.7008895024938403, Validation Loss: 1.056671811313164\n",
      "Training Accuracy: 0.6887052341597796, Training F1: 0.6827199861740928\n",
      "Validation Accuracy: 0.42923794712286156, Validation F1: 0.4465480760284447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 159/159 [01:10<00:00,  2.27it/s]\n",
      "Validation Epoch 9/10: 100%|██████████| 41/41 [00:09<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.6361420984912969, Validation Loss: 1.4946514149991477\n",
      "Training Accuracy: 0.7221566312475404, Training F1: 0.7178940786032271\n",
      "Validation Accuracy: 0.38724727838258166, Validation F1: 0.36856376668803176\n",
      "Stopping early due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 0\n",
    "for epoch in range(epochs):\n",
    "    # 訓練階段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_train_preds = []\n",
    "    all_train_targets = []\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        images = images.to('cuda')\n",
    "        targets = targets.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 收集預測和目標\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_train_preds.extend(preds.cpu().numpy())\n",
    "        all_train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    # 驗證階段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_preds = []\n",
    "    all_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "            images = images.to('cuda')\n",
    "            targets = targets.to('cuda')\n",
    "            outputs = model(images)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets.long())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # 收集預測和目標\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    # 計算準確度和 F1 分數\n",
    "    train_accuracy = accuracy_score(all_train_targets, all_train_preds)\n",
    "    train_f1 = f1_score(all_train_targets, all_train_preds, average='weighted')\n",
    "    val_accuracy = accuracy_score(all_val_targets, all_val_preds)\n",
    "    val_f1 = f1_score(all_val_targets, all_val_preds, average='weighted')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}, Training F1: {train_f1}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy}, Validation F1: {val_f1}\")\n",
    "\n",
    "    # 保存模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    # 早期停止\n",
    "    if patience > 3:\n",
    "        print(\"Stopping early due to no improvement in validation loss.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
